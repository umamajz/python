# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11cXXr0cpihRrxcMnYcuS0jgyTGTogaP0

A
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Define the autoencoder architecture
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Load and preprocess the image
image_path = '/content/image.png'
image = Image.open(image_path).convert('RGB')
image = np.array(image)
image = torch.from_numpy(image).type(torch.FloatTensor)
image = np.transpose(image, (2, 0, 1))
image = np.expand_dims(image, axis=0)
image = torch.from_numpy(image)

# Train the autoencoder
autoencoder = Autoencoder()
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)
num_epochs = 10

# Define a transformation to convert the tensor back to an image
to_pil_image = transforms.ToPILImage()

for epoch in range(num_epochs):
    output = autoencoder(image)
    loss = criterion(output, image)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

    # Convert the output tensor to an image and display it
    output_image = output.detach().numpy()
    output_image = np.squeeze(output_image, axis=0)
    output_image = np.transpose(output_image, (1, 2, 0))
    output_image = (output_image * 255).astype(np.uint8)  # Convert to integer data type
    output_image = to_pil_image(output_image)
    plt.imshow(output_image)
    plt.show()

"""Additional Details"""

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# Define the autoencoder architecture
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Load and preprocess the image
image_path = '/content/image.png'
image = Image.open(image_path).convert('RGB')
image = np.array(image)
image = torch.from_numpy(image).type(torch.FloatTensor)
image = np.transpose(image, (2, 0, 1))
image = np.expand_dims(image, axis=0)
image = torch.from_numpy(image)

# Load the trained model weights
model_path = '/content/autoencoder.pth'
autoencoder = Autoencoder()
#autoencoder.load_state_dict(torch.load(model_path))

# Evaluate the model on the image
with torch.no_grad():
    output = autoencoder(image)
    loss = nn.MSELoss()(output, image)
    mae = nn.L1Loss()(output, image)
    rmse = torch.sqrt(nn.MSELoss()(output, image))
    mse = loss**2
    r_squared = 1 - loss/(torch.var(image) + 1e-8)

    print('Loss: {:.4f}'.format(loss.item()))
    print('MAE: {:.4f}'.format(mae.item()))
    print('RMSE: {:.4f}'.format(rmse.item()))
    print('MSE: {:.4f}'.format(mse.item()))
    print('R-squared: {:.4f}'.format(r_squared.item()))

# Save the trained model weights
model_path = '/content/autoencoder.pth'
torch.save(autoencoder.state_dict(), model_path)

"""B"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder1 = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.encoder2 = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.decoder1 = nn.Sequential(
            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )
        self.decoder2 = nn.Sequential(
            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )
        self.fc1 = nn.Linear(16 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, 1)

def forward(self, x1, x2):
        x1 = self.encoder1(x1)
        x2 = self.encoder2(x2)
        x = torch.cat((x1, x2), dim=1)
        x = self.decoder1(x)
        x = self.decoder2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x

from PIL import Image
import numpy as np

image_path1 = '/content/image1.png'
image_path2 = '/content/image2.png'
image1 = Image.open(image_path1).convert('RGB')
image2 = Image.open(image_path2).convert('RGB')
image1 = np.array(image1)
image2 = np.array(image2)
image1 = torch.from_numpy(image1).type(torch.FloatTensor)
image2 = torch.from_numpy(image2).type(torch.FloatTensor)
image1 = np.transpose(image1, (2, 0, 1))
image2 = np.transpose(image2, (2, 0, 1))
image1 = np.expand_dims(image1, axis=0)
image2 = np.expand_dims(image2, axis=0)

learning_rate = 0.001

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

print(image1_tensor.shape)
print(image2_tensor.shape)

num_epochs = 10

for epoch in range(num_epochs):
    # Convert numpy arrays to PyTorch tensors
    image1_tensor = torch.from_numpy(image1)
    image2_tensor = torch.from_numpy(image2)


    # Reshape input tensors to have the correct dimensions
    #image1_tensor = torch.from_numpy(image1).unsqueeze(0)
    #image2_tensor = torch.from_numpy(image2).unsqueeze(0)


    # Forward pass
    outputs = model(image1_tensor, image2_tensor)

    # Compute loss
    loss = criterion(outputs, image1_tensor)

    # Backward pass and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print loss
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

